{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-jQj4orSOia",
        "outputId": "7c672472-12dd-446b-b9ba-5e39783b1c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_GEwsTGTjvD",
        "outputId": "f33942ab-b829-4e0b-9225-b655e4c2fb84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import re"
      ],
      "metadata": {
        "id": "fC46kmK7SbtA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(500)"
      ],
      "metadata": {
        "id": "50v_vSd3Sg15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(input):\n",
        "  remove_symbols = re.compile('[^0-9A-Za-z ]') \n",
        "  input = remove_symbols.sub(' ', str(input)) \n",
        "  # remove  empty spaces\n",
        "  re.sub('\\s+', '' ,input)\n",
        "  return input"
      ],
      "metadata": {
        "id": "eEaOZtN1t-l4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/train/train.En.csv')\n",
        "df.head(10)\n",
        "\n",
        "df_extracted = pd.read_csv('/content/drive/MyDrive/Dataset/train/sarcastic_tweets.csv')\n",
        "df_extracted.drop(columns=df_extracted.columns[0], axis=1, inplace=True)\n",
        "# Extract the sarcastic tweets only\n",
        "df_sarcastic = df_extracted[df_extracted['sarcastic']==1]\n",
        "# Extract the non sarcastic tweets from the original training dataset \n",
        "df_not_sarcastic = df[df['sarcastic'] == 0][['tweet', 'sarcastic']]\n",
        "# Extract the non sarcastic rephrase of the sarcastic tweets\n",
        "rephrase = df_extracted[df_extracted['sarcastic'] == 0]\n",
        "# Concatenate all non sarcastic tweets in a single dataframe\n",
        "# df_not_sarcastic = pd.concat([df_not_sarcastic,rephrase],axis=0, ignore_index=True )\n",
        "\n",
        "count = df_sarcastic.shape[0]\n",
        "\n",
        "# Oversample the sarcastic tweets\n",
        "df_sarcastic_over = df_sarcastic.sample(count * 4, replace=True)\n",
        "\n",
        "# # Undersample the non sarcastic tweets\n",
        "# df_not_sarcastic_under = df_not_sarcastic.sample(count, replace=False)\n",
        "\n",
        "#  Concatenate the sarcastic tweets (oversampled), the rephrase and the non sarcastic tweets (undersampled)\n",
        "df = pd.concat([df_sarcastic_over, rephrase, df_not_sarcastic],axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "figoXDZ-SmBi"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "SsKnN2ICXXOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any non alphanumeric characters and any trailing white spaces\n",
        "df['tweet'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "df['tweet'] = df['tweet'].astype(str)\n",
        "# lower case all letters\n",
        "df['tweet'] = [entry.lower() for entry in df['tweet']]\n",
        "# break tweets into a set of words\n",
        "df['tweet']= [word_tokenize(entry) for entry in df['tweet']]"
      ],
      "metadata": {
        "id": "WTaHzVgeQ5Tl"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/Dataset/test/Task-A/task_A_En_test.csv') \n",
        "\n",
        " # Remove any non alphanumeric characters and any trailing white spaces\n",
        "df_test['tweet'] = df_test['tweet'].apply(clean_text)\n",
        "\n",
        "df_test['tweet'] = df_test['tweet'].astype(str)\n",
        "# lower case all letters\n",
        "df_test['tweet'] = [entry.lower() for entry in df_test['tweet']]\n",
        "# break tweets into a set of words\n",
        "df_test['tweet']= [word_tokenize(entry) for entry in df_test['tweet']]"
      ],
      "metadata": {
        "id": "ZNjosxGMksrV"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "q4HshSVTPUDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV"
      ],
      "metadata": {
        "id": "YDDjy1FhNCEi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, element in enumerate(df['tweet']):\n",
        "  final_words = []\n",
        "  word_lemmatized = WordNetLemmatizer()\n",
        "  for word, tag in pos_tag(element):\n",
        "    if word not in stopwords.words('english') and word.isalpha():\n",
        "      word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]])\n",
        "      final_words.append(word_final)\n",
        "  df.loc[index, 'tweet_final'] = str(final_words)\n",
        "\n",
        "for index, element in enumerate(df_test['tweet']):\n",
        "  test_final_words = []\n",
        "  word_lemmatized = WordNetLemmatizer()\n",
        "  for word, tag in pos_tag(element):\n",
        "    if word not in stopwords.words('english') and word.isalpha():\n",
        "      test_word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]])\n",
        "      test_final_words.append(test_word_final)\n",
        "  df_test.loc[index, 'tweet_final'] = str(test_final_words)"
      ],
      "metadata": {
        "id": "hGhU7A-QTyjs"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(df['tweet_final'])\n",
        "\n",
        "train_x = tfidf_vect.transform(df['tweet_final'])\n",
        "test_x = tfidf_vect.transform(df_test['tweet_final'])"
      ],
      "metadata": {
        "id": "K7Yd2CXKUYeg"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect_test = TfidfVectorizer()\n",
        "tfidf_vect_test.fit(df_test['tweet_final'])\n",
        "print(len(tfidf_vect_test.vocabulary_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-XhemRly-Sy",
        "outputId": "0cb3b985-16a7-4fb7-d7c6-f55ffa64968b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tfidf_vect.vocabulary_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lk7twjFviAe",
        "outputId": "a74624e1-c01c-49dc-ef70-40b9abeda860"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "present = []\n",
        "absent = []\n",
        "for key in tfidf_vect_test.vocabulary_:\n",
        "  if key in tfidf_vect.vocabulary_:\n",
        "    count = count + 1\n",
        "    present.append(key)\n",
        "  else:\n",
        "    absent.append(key)\n",
        "\n",
        "count\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj7X-yW-zkvc",
        "outputId": "86dba28f-80ec-48f3-ca40-47d9d1d5e324"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2175"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf_vect.vocabulary_\n",
        "len(absent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW_mE8ZSY_1h",
        "outputId": "2964d302-cd65-4e4f-b005-9ec994075a5d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1362"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = svm.SVC(C=5.0, kernel='sigmoid')\n",
        "SVM.fit(train_x, df['sarcastic'])\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(test_x)\n",
        "print(classification_report(df_test['sarcastic'],predictions_SVM))\n",
        "# # Use accuracy_score function to get the accuracy\n",
        "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGL2_5-fkay9",
        "outputId": "a3f8f4ca-2f04-4b9f-f281-ce95132f7e83"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.68      0.77      1200\n",
            "           1       0.21      0.51      0.30       200\n",
            "\n",
            "    accuracy                           0.66      1400\n",
            "   macro avg       0.55      0.60      0.54      1400\n",
            "weighted avg       0.80      0.66      0.71      1400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(df_test['sarcastic'],predictions_SVM))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rljbYNo0qSrm",
        "outputId": "d4bab4f4-5c7f-4f2e-f0c9-b0a30899139f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[819 381]\n",
            " [ 98 102]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv('/content/drive/MyDrive/Dataset/test/df.csv')\n",
        "# df_test.to_csv('/content/drive/MyDrive/Dataset/test/df_test.csv')\n",
        "df_results = pd.DataFrame()\n",
        "df_results['True-Value'] = df_test['sarcastic']\n",
        "df_results['Prediction'] = predictions_SVM\n",
        "df_results.head(10)\n",
        "df_test.to_csv('/content/drive/MyDrive/Dataset/test/Task-A/task-a-svm-oversampled.csv')"
      ],
      "metadata": {
        "id": "xKDTGjgItbej"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}