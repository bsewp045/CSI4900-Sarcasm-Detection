{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK0Vk16QFmkP",
        "outputId": "c6353f1d-0138-4b19-feea-0b08817fcbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rkWAnzUuFnu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3bef13-d493-4974-b7fb-d31618dfa6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.1.98)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.13.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_ytmUnbDFrMI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z2syT-S0t1CH"
      },
      "outputs": [],
      "source": [
        "# class RobertaClassifier(torch.nn.Module):\n",
        "#     def __init__(self, dropout_rate=0.3):\n",
        "#         super(RobertaClassifier, self).__init__()\n",
        "        \n",
        "#         self.model = RobertaForSequenceClassification.from_pretrained('roberta-base', output_hidden_states = True, num_labels=2)\n",
        "#         self.model.roberta.config.type_vocab_size = 2 \n",
        "#         # single_emb = self.model.roberta.embeddings.token_type_embeddings\n",
        "#         # self.model.roberta.embeddings.token_type_embeddings = torch.nn.Embedding(2, single_emb.embedding_dim)\n",
        "#         # self.roberta.embeddings.token_type_embeddings.weight = torch.nn.Parameter(single_emb.weight.repeat([2, 1]))\n",
        "\n",
        "#         # self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "#         # self.l1 = torch.nn.Linear(768, 2)\n",
        "        \n",
        "#     def forward(self, input_ids, attention_mask,token_type_ids,labels):\n",
        "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,labels = labels)\n",
        "#         return output\n",
        "#         # last_cls = output['last_hidden_state'][:,0,:]\n",
        "#         # x = self.d1(last_cls)\n",
        "#         # x = self.l1(x)\n",
        "#         # return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EnbuyNoqS5ao"
      },
      "outputs": [],
      "source": [
        "def load_data(input_id, attention_masks,token_type_ids, labels, batch_size = 64):\n",
        "  train_set = TensorDataset(input_id, \n",
        "                          attention_masks, \n",
        "                          token_type_ids,\n",
        "                          labels)\n",
        "  train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "  return train_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aNNwcEuuStSK"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(train_dataloader, epochs=4):\n",
        "    \"\"\"Initialize the Roberta Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', output_hidden_states = True, num_labels=2)\n",
        "    model.roberta.config.type_vocab_size = 3\n",
        "    single_emb = model.roberta.embeddings.token_type_embeddings\n",
        "    model.roberta.embeddings.token_type_embeddings = torch.nn.Embedding(3, single_emb.embedding_dim)\n",
        "    roberta_classifier = model\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(roberta_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return roberta_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CB8EpaEVTDjF"
      },
      "outputs": [],
      "source": [
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_dataloader,test_dataloader,optimizer,df_results, scheduler,epochs=4):\n",
        "  \"\"\"Train the roberta classifier model.\n",
        "  \"\"\"\n",
        "  # Start training loop\n",
        "  print(\"Start training...\\n\")\n",
        "  for epoch_i in range(epochs):\n",
        "    model_save_name = 'task-c-roberta-token-id.pt'\n",
        "    col_name = 'roberta-base-token-id2-epoch-' + str(epoch_i+4)\n",
        "\n",
        "    # =======================================\n",
        "    #               Training\n",
        "    # =======================================\n",
        "\n",
        "    # Reset tracking variables at the beginning of each epoch\n",
        "    # total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Put the model into the training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      # Load batch to GPU\n",
        "      batch = tuple(t for t in batch)\n",
        "      b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "\n",
        "      # Zero out any previously calculated gradients\n",
        "      model.zero_grad()\n",
        "      # Perform a forward pass. This will return logits.\n",
        "      output = model(input_ids = b_input_ids, attention_mask = b_input_mask, token_type_ids = b_token_type_ids,labels=b_labels)\n",
        "\n",
        "      # Compute loss and accumulate the loss values\n",
        "      # loss = loss_fn(output, b_labels)\n",
        "      loss = output.loss\n",
        "      tr_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "      \n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Update parameters and the learning rate\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "    model_save_name = 'epoch-' + str(epoch_i+4) + '-' + model_save_name\n",
        "    path = F\"/content/drive/MyDrive/Dataset/train/Task-C/{model_save_name}\" \n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "    predictions = roberta_predict(model, test_dataloader)\n",
        "    path = \"/content/drive/MyDrive/Dataset/test/Task-C/\" + col_name + \".csv\"\n",
        "    df_results[col_name] = predictions\n",
        "    df_results.to_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KDHV5-mGju1A"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def roberta_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attention_mask, b_token_type_ids,b_labels = tuple(t for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            output = model(b_input_ids, b_attention_mask, token_type_ids = b_token_type_ids, labels=b_labels)\n",
        "        # all_logits.append(output)\n",
        "        all_logits.append(output.logits)\n",
        "\n",
        "        \n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().detach().numpy()\n",
        "    predictions = np.argmax(probs, axis=1)\n",
        "    # threshold = 0.5\n",
        "    # preds = np.where(probs.iloc[:, 1] > threshold, 1, 0)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tJzFALe5u75b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ube9hbJWIj6k"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/train/sarcastic_tweets.csv')\n",
        "\n",
        "# Pre-processing\n",
        "remove_symbols = re.compile('[^0-9A-Za-z ]') \n",
        "\n",
        "def clean_text(input):\n",
        "  input = remove_symbols.sub(' ', str(input)) \n",
        "  # remove  empty spaces\n",
        "  re.sub('\\s+', '' ,input)\n",
        "  return input\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zhxi52pDItU-"
      },
      "outputs": [],
      "source": [
        "df_sarcastic = df[df['sarcastic'] == 1 ][['tweet','sarcastic']]\n",
        "df_rephrase = df[df['sarcastic'] == 0 ][['tweet', 'sarcastic']]\n",
        "\n",
        "text_sarcastic = df_sarcastic.tweet.values\n",
        "# labels_sarcastic = df_sarcastic.sarcastic.values\n",
        "\n",
        "text_rephrase = df_rephrase.tweet.values\n",
        "# labels_rephrase = df_rephrase.sarcastic.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_token_type_id(input_ids, pad_length = 200):\n",
        "#   token_type_id = []\n",
        "#   input_ids = input_ids.squeeze().numpy()\n",
        "#   token_two_1 = np.where(input_ids == 2)[0][0] + 1\n",
        "#   token_two_3 = np.where(input_ids == 2)[0][2] + 1\n",
        "#   padding = pad_length - token_two_3 \n",
        "#   for i in range(token_two_1):\n",
        "#     token_type_id.append(0)\n",
        "#   for i in range(token_two_1,token_two_3):\n",
        "#     token_type_id.append(1)\n",
        "#   for i in range(token_two_3,200):\n",
        "#     token_type_id.append(0)\n",
        "#   return(torch.Tensor([token_type_id]).to(torch.int32))\n",
        "  "
      ],
      "metadata": {
        "id": "cBj0aoRRT58G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_token_type_id(input_ids, pad_length = 200):\n",
        "  token_type_id = []\n",
        "  input_ids = input_ids.squeeze().numpy()\n",
        "  token_two_1 = np.where(input_ids == 2)[0][0] + 1\n",
        "  token_two_3 = np.where(input_ids == 2)[0][2] + 1\n",
        "  padding = pad_length - token_two_3 \n",
        "  for i in range(token_two_1):\n",
        "    token_type_id.append(0)\n",
        "  for i in range(token_two_1,token_two_3):\n",
        "    token_type_id.append(1)\n",
        "  for i in range(token_two_3,200):\n",
        "    token_type_id.append(2)\n",
        "  return(torch.Tensor([token_type_id]).to(torch.int32))"
      ],
      "metadata": {
        "id": "Y19OOQAE1qUp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)"
      ],
      "metadata": {
        "id": "tIKSBv0idXyc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM3ktInKHIsB",
        "outputId": "f0cb4dac-8e6b-4b87-d6c3-afcf49f92271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "input_id = []\n",
        "token_type_id = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "\n",
        "for element1, element2 in zip(text_sarcastic, text_rephrase):\n",
        "  encoded_dict_0 = tokenizer(element1, element2, max_length=200,pad_to_max_length=True,return_tensors = 'pt')\n",
        "  encoded_dict_1 = tokenizer(element2, element1, max_length=200,pad_to_max_length=True,return_tensors = 'pt')\n",
        "  input_id.append(encoded_dict_0['input_ids'])\n",
        "  input_id.append(encoded_dict_1['input_ids'])\n",
        "  token_type_id.append(create_token_type_id(encoded_dict_0['input_ids']))\n",
        "  token_type_id.append(create_token_type_id(encoded_dict_1['input_ids']))\n",
        "  attention_masks.append(encoded_dict_0['attention_mask'])\n",
        "  attention_masks.append(encoded_dict_1['attention_mask'])\n",
        "  labels.append(0)\n",
        "  labels.append(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = torch.cat(input_id, dim = 0)\n",
        "token_type_id = torch.cat(token_type_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "F9b8LVvkevuv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpPBILsAUrXP",
        "outputId": "0a03a68b-0af3-44ff-ec3b-c27de8237c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/Dataset/test/task_C_En_test.csv')\n",
        "\n",
        "df_test['text_0'] = df_test['text_0'].apply(clean_text)\n",
        "df_test['text_1'] = df_test['text_1'].apply(clean_text)\n",
        "\n",
        "text_0 = df_test.text_0.values\n",
        "text_1 = df_test.text_1.values\n",
        "\n",
        "test_input_id = []\n",
        "test_token_type_id = []\n",
        "test_attention_masks = []\n",
        "for element1, element2 in zip(text_0, text_1):\n",
        "  encoded_dict = tokenizer(element1, element2, max_length=200,pad_to_max_length=True,return_tensors = 'pt')\n",
        "  test_input_id.append(encoded_dict['input_ids'])\n",
        "  test_token_type_id.append(create_token_type_id(encoded_dict['input_ids']))\n",
        "  test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ViiTFSgyUx2i"
      },
      "outputs": [],
      "source": [
        "test_input_id = torch.cat(test_input_id, dim = 0)\n",
        "test_token_type_id = torch.cat(test_token_type_id, dim = 0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim = 0)\n",
        "test_labels = torch.tensor(df_test['sarcastic_id'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_id.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29dhvSMBegHT",
        "outputId": "cc5f3198-6d70-47cd-b0bf-9ee02f311d39"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7F6BSqfmhsjp",
        "outputId": "d6ccda66-f83c-46dd-ed05-78b737070b49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              text_0  \\\n",
              "0           I see that your team played well today     \n",
              "1  Anthony Taylor is such a fair referee  I wish ...   \n",
              "2     the weather is gloomy  just raining and dull     \n",
              "3  People going out to get there boosters without...   \n",
              "4  Really great weather we re having  love a bit ...   \n",
              "\n",
              "                                              text_1  sarcastic_id  \n",
              "0     I m sorry that your team didn t win yesterday              0  \n",
              "1  I hope Anthony Taylor is never put in charge o...             0  \n",
              "2                     What a glorious weather today              1  \n",
              "3  Nice to see the sheep getting their boosters t...             1  \n",
              "4  Really cold January so far   looking forward t...             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d91d64e6-65e9-409b-826d-006f377d0c95\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_0</th>\n",
              "      <th>text_1</th>\n",
              "      <th>sarcastic_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I see that your team played well today</td>\n",
              "      <td>I m sorry that your team didn t win yesterday</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anthony Taylor is such a fair referee  I wish ...</td>\n",
              "      <td>I hope Anthony Taylor is never put in charge o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the weather is gloomy  just raining and dull</td>\n",
              "      <td>What a glorious weather today</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>People going out to get there boosters without...</td>\n",
              "      <td>Nice to see the sheep getting their boosters t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Really great weather we re having  love a bit ...</td>\n",
              "      <td>Really cold January so far   looking forward t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d91d64e6-65e9-409b-826d-006f377d0c95')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d91d64e6-65e9-409b-826d-006f377d0c95 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d91d64e6-65e9-409b-826d-006f377d0c95');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T185Kk4yeb8X",
        "outputId": "0d97cf7b-02db-4952-9476-f7a8f056b4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_dataloader =  load_data(input_id,attention_masks,token_type_id,labels)\n",
        "test_dataloader = load_data(test_input_id,test_attention_masks,test_token_type_id ,test_labels)\n",
        "\n",
        "df_results = pd.DataFrame()\n",
        "df_results['True-Value'] = df_test['sarcastic_id'].values\n",
        "\n",
        "robert_classifier, optimizer, scheduler = initialize_model(train_dataloader)\n",
        "#train(robert_classifier, train_dataloader, test_dataloader, optimizer,df_results, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "robert_classifier, optimizer, scheduler = initialize_model(train_dataloader)\n",
        "robert_classifier.load_state_dict(torch.load(\"/content/drive/MyDrive/Dataset/train/Task-C/epoch-4-task-c-roberta-token-id.pt\"))\n",
        "train(robert_classifier, train_dataloader, test_dataloader, optimizer,df_results, scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU-kVdpeAe2f",
        "outputId": "c519141c-6a8b-40e2-8c48-b4e51a293f85"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            "Training complete!\n",
            "Training complete!\n",
            "Training complete!\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "-zVgU7lkwyJv",
        "outputId": "cc4570db-be88-4b5e-ac91-48eedca40b59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0  True-Value  roberta-base-token-id2-epoch-1  \\\n",
              "0            0           0                               1   \n",
              "1            1           0                               1   \n",
              "2            2           1                               1   \n",
              "3            3           1                               1   \n",
              "4            4           0                               1   \n",
              "5            5           1                               1   \n",
              "6            6           0                               1   \n",
              "7            7           1                               1   \n",
              "8            8           0                               1   \n",
              "9            9           0                               1   \n",
              "10          10           1                               1   \n",
              "11          11           1                               1   \n",
              "12          12           1                               1   \n",
              "13          13           1                               1   \n",
              "14          14           1                               1   \n",
              "\n",
              "    roberta-base-token-id2-epoch-2  roberta-base-token-id2-epoch-3  \n",
              "0                                0                               1  \n",
              "1                                0                               1  \n",
              "2                                0                               0  \n",
              "3                                1                               1  \n",
              "4                                1                               0  \n",
              "5                                1                               0  \n",
              "6                                0                               1  \n",
              "7                                1                               1  \n",
              "8                                1                               1  \n",
              "9                                1                               1  \n",
              "10                               1                               0  \n",
              "11                               0                               1  \n",
              "12                               0                               1  \n",
              "13                               1                               1  \n",
              "14                               0                               1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3d75ff3-6d46-45e8-8b8f-91a19f766924\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>True-Value</th>\n",
              "      <th>roberta-base-token-id2-epoch-1</th>\n",
              "      <th>roberta-base-token-id2-epoch-2</th>\n",
              "      <th>roberta-base-token-id2-epoch-3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3d75ff3-6d46-45e8-8b8f-91a19f766924')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3d75ff3-6d46-45e8-8b8f-91a19f766924 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3d75ff3-6d46-45e8-8b8f-91a19f766924');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "pred = pd.read_csv(\"/content/drive/MyDrive/Dataset/test/Task-C/roberta-base-token-id2-epoch-3.csv\", sep=\",\")\n",
        "# prob.drop(index=prob.index[0], axis=0, inplace=True)\n",
        "# prob.head(2)\n",
        "# threshold = 0.5\n",
        "# preds = np.where(prob.iloc[:, 1] > threshold, 1, 0)\n",
        "# preds\n",
        "pred.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGC2NwCCw2nl",
        "outputId": "17cf6b4b-932b-46b3-b4ab-2638b0367451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[33 74]\n",
            " [36 57]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(pred['True-Value'], pred['roberta-base-token-id2-epoch-3']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytGjKSRjw5fy",
        "outputId": "91a34b2e-34b5-45bf-a77b-1012e7b4f7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.31      0.37       107\n",
            "           1       0.44      0.61      0.51        93\n",
            "\n",
            "    accuracy                           0.45       200\n",
            "   macro avg       0.46      0.46      0.44       200\n",
            "weighted avg       0.46      0.45      0.44       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(pred['True-Value'], pred['roberta-base-token-id2-epoch-3']))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', output_hidden_states = True)\n",
        "# model.config.type_vocab_size = 2 \n",
        "# single_emb = model.embeddings.token_type_embeddings\n",
        "# single_emb.embedding_dim\n",
        "#self.roberta.embeddings.token_type_embeddings = torch.nn.Embedding(2, single_emb.embedding_dim)"
      ],
      "metadata": {
        "id": "gtbLzqssqUGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.roberta.embeddings.token_type_embeddings\n"
      ],
      "metadata": {
        "id": "PFv90hw5tt8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}