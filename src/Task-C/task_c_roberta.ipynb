{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK0Vk16QFmkP",
        "outputId": "8e05a235-baee-4d8d-d3ee-59a39b53c353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "rkWAnzUuFnu_"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_ytmUnbDFrMI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "z2syT-S0t1CH"
      },
      "outputs": [],
      "source": [
        "class RobertaClassifier(torch.nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(RobertaClassifier, self).__init__()\n",
        "        \n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base', output_hidden_states = True)\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(768, 2)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_last_hidden = output['last_hidden_state'][:,0,:]\n",
        "        x = self.d1(cls_last_hidden)\n",
        "        x = self.l1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "EnbuyNoqS5ao"
      },
      "outputs": [],
      "source": [
        "def load_data(input_id, attention_masks, labels, batch_size = 64):\n",
        "  train_set = TensorDataset(input_id, \n",
        "                          attention_masks, \n",
        "                          labels)\n",
        "  train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "  return train_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "aNNwcEuuStSK"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(train_dataloader, epochs=4):\n",
        "    \"\"\"Initialize the Roberta Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    roberta_classifier = RobertaClassifier()\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(roberta_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return roberta_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "CB8EpaEVTDjF"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_dataloader,test_dataloader,optimizer,df_results, scheduler,epochs=4):\n",
        "  \"\"\"Train the roberta classifier model.\n",
        "  \"\"\"\n",
        "  # Start training loop\n",
        "  print(\"Start training...\\n\")\n",
        "  for epoch_i in range(epochs):\n",
        "    model_save_name = 'task-c-roberta.pt'\n",
        "    col_name = 'roberta-base-epoch-' + str(epoch_i+1)\n",
        "\n",
        "    # =======================================\n",
        "    #               Training\n",
        "    # =======================================\n",
        "\n",
        "    # Reset tracking variables at the beginning of each epoch\n",
        "    # total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Put the model into the training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      # Load batch to GPU\n",
        "      batch = tuple(t for t in batch)\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "      # Zero out any previously calculated gradients\n",
        "      model.zero_grad()\n",
        "      # Perform a forward pass. This will return logits.\n",
        "      output = model(input_ids = b_input_ids, attention_mask = b_input_mask)s\n",
        "\n",
        "      # Compute loss and accumulate the loss values\n",
        "      loss = loss_fn(output, b_labels)\n",
        "      tr_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "      \n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Update parameters and the learning rate\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "    model_save_name = 'epoch-' + str(epoch_i+1) + '-' + model_save_name\n",
        "    path = F\"/content/drive/MyDrive/Dataset/train/Task-C/{model_save_name}\" \n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "    predictions = roberta_predict(model, test_dataloader)\n",
        "    path = \"/content/drive/MyDrive/Dataset/test/Task-C/\" + col_name + \".csv\"\n",
        "    df_results[col_name] = predictions\n",
        "    df_results.to_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "KDHV5-mGju1A"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def roberta_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attention_mask,_ = tuple(t for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            output = model(b_input_ids, b_attention_mask)\n",
        "        all_logits.append(output)\n",
        "\n",
        "        \n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().detach().numpy()\n",
        "    predictions = np.argmax(probs, axis=1)\n",
        "    # threshold = 0.5\n",
        "    # preds = np.where(probs.iloc[:, 1] > threshold, 1, 0)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ube9hbJWIj6k"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/train/sarcastic_tweets.csv')\n",
        "\n",
        "# Pre-processing\n",
        "remove_symbols = re.compile('[^0-9A-Za-z ]') \n",
        "\n",
        "def clean_text(input):\n",
        "  input = remove_symbols.sub(' ', str(input)) \n",
        "  # remove  empty spaces\n",
        "  re.sub('\\s+', '' ,input)\n",
        "  return input\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "zhxi52pDItU-"
      },
      "outputs": [],
      "source": [
        "df_sarcastic = df[df['sarcastic'] == 1 ][['tweet','sarcastic']]\n",
        "df_rephrase = df[df['sarcastic'] == 0 ][['tweet', 'sarcastic']]\n",
        "\n",
        "text_sarcastic = df_sarcastic.tweet.values\n",
        "# labels_sarcastic = df_sarcastic.sarcastic.values\n",
        "\n",
        "text_rephrase = df_rephrase.tweet.values\n",
        "# labels_rephrase = df_rephrase.sarcastic.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "GlJw71iSSLOL"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM3ktInKHIsB",
        "outputId": "232bc1b5-ee7c-4f95-a3e6-5f4b72a6c8b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "input_id = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "\n",
        "for element1, element2 in zip(text_sarcastic, text_rephrase):\n",
        "  encoded_dict_0 = tokenizer(element1, element2, max_length=200,pad_to_max_length=True,return_tensors = 'pt')\n",
        "  encoded_dict_1 = tokenizer(element2, element1, max_length=200,pad_to_max_length=True,return_tensors = 'pt')\n",
        "  input_id.append(encoded_dict_0['input_ids'])\n",
        "  input_id.append(encoded_dict_1['input_ids'])\n",
        "  attention_masks.append(encoded_dict_0['attention_mask'])\n",
        "  attention_masks.append(encoded_dict_1['attention_mask'])\n",
        "  labels.append(0)\n",
        "  labels.append(1)\n",
        "\n",
        "\n",
        "input_id = torch.cat(input_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpPBILsAUrXP",
        "outputId": "9a52486d-77da-496d-9ea8-0677101f6c2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/Dataset/test/task_C_En_test.csv')\n",
        "text_0 = df_test.text_0.values\n",
        "text_1 = df_test.text_1.values\n",
        "\n",
        "test_input_id = []\n",
        "test_attention_masks = []\n",
        "for element1, element2 in zip(text_0, text_1):\n",
        "  encoded_dict = tokenizer(element1, element2, max_length=200,pad_to_max_length=True,return_tensors = 'pt')\n",
        "  test_input_id.append(encoded_dict['input_ids'])\n",
        "  test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ViiTFSgyUx2i"
      },
      "outputs": [],
      "source": [
        "test_input_id = torch.cat(test_input_id, dim = 0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim = 0)\n",
        "test_labels = torch.tensor(df_test['sarcastic_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T185Kk4yeb8X",
        "outputId": "eab2acc9-9ba3-4f7b-e4d8-fad7b2bf1425"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataloader =  load_data(input_id,attention_masks,labels)\n",
        "test_dataloader = load_data(test_input_id,test_attention_masks,test_labels)\n",
        "\n",
        "df_results = pd.DataFrame()\n",
        "df_results['True-Value'] = df_test['sarcastic_id'].values\n",
        "\n",
        "robert_classifier, optimizer, scheduler = initialize_model(train_dataloader)\n",
        "output = train(robert_classifier, train_dataloader, test_dataloader, optimizer,df_results, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-zVgU7lkwyJv",
        "outputId": "739583ab-1f3e-4cf1-e040-bdbcc9386873"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5ac919d9-71bc-421a-ae1f-0ed5b91c3622\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>True-Value</th>\n",
              "      <th>roberta-base-epoch-1</th>\n",
              "      <th>roberta-base-epoch-2</th>\n",
              "      <th>roberta-base-epoch-3</th>\n",
              "      <th>roberta-base-epoch-4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ac919d9-71bc-421a-ae1f-0ed5b91c3622')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ac919d9-71bc-421a-ae1f-0ed5b91c3622 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ac919d9-71bc-421a-ae1f-0ed5b91c3622');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  True-Value  roberta-base-epoch-1  roberta-base-epoch-2  \\\n",
              "0           0           0                     0                     0   \n",
              "1           1           0                     1                     1   \n",
              "2           2           1                     0                     0   \n",
              "3           3           1                     1                     0   \n",
              "4           4           0                     1                     1   \n",
              "\n",
              "   roberta-base-epoch-3  roberta-base-epoch-4  \n",
              "0                     0                     0  \n",
              "1                     1                     0  \n",
              "2                     0                     0  \n",
              "3                     0                     1  \n",
              "4                     1                     1  "
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "pred = pd.read_csv(\"/content/drive/MyDrive/Dataset/test/Task-C/roberta-base-epoch-4.csv\", sep=\",\")\n",
        "# prob.drop(index=prob.index[0], axis=0, inplace=True)\n",
        "# prob.head(2)\n",
        "# threshold = 0.5\n",
        "# preds = np.where(prob.iloc[:, 1] > threshold, 1, 0)\n",
        "# preds\n",
        "pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGC2NwCCw2nl",
        "outputId": "6caadf78-71ba-4831-ffe6-0fa7b05f9614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[69 38]\n",
            " [57 36]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(pred['True-Value'], pred['roberta-base-epoch-3']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytGjKSRjw5fy",
        "outputId": "27be321d-88ce-4e2a-8bd0-2eca1941b66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.64      0.59       107\n",
            "           1       0.49      0.39      0.43        93\n",
            "\n",
            "    accuracy                           0.53       200\n",
            "   macro avg       0.52      0.52      0.51       200\n",
            "weighted avg       0.52      0.53      0.52       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(pred['True-Value'], pred['roberta-base-epoch-3']))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
